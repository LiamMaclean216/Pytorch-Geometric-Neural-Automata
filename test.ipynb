{"cells":[{"cell_type":"markdown","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":112977,"status":"ok","timestamp":1666709139614,"user":{"displayName":"Liam Maclean","userId":"16471805810852624957"},"user_tz":240},"id":"ZXja8iz8lEON","outputId":"cc7033ca-6ef4-4c25-e788-cb6dcafee493"},"source":["!conda install --file /content/drive/MyDrive/GitHub/Pytorch-Geometric-Neural-Automata/requirements.txt\n","import sys\n","sys.path.insert(0,'/content/drive/MyDrive/GitHub/Pytorch-Geometric-Neural-Automata/')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# conda install pytorch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0 -c pytorch -y & conda install pyg -c pyg -y & conda install -c anaconda networkx -y & conda install -c conda-forge matplotlib -y\n","import wandb\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","from torch_geometric.data import Data\n","import networkx as nx\n","import matplotlib.pyplot as plt\n","import torch_geometric.utils as utils\n","from utils import *\n","from model import UpdateRule\n","from datasets import *\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["seed(12345)\n","d_len = 4\n","dataset = load_arc(print_filenames=True)\n","\n","def process_arc_sample(sample, io, final_dim=3, pad = False):\n","    sample = torch.tensor([x[io] for x in sample['train'] + sample['test']])\n","    if pad:\n","        sample = F.pad(sample, (0, final_dim - sample.shape[2], 0, final_dim - sample.shape[1]))\n","    sample = one_hot(sample.flatten(1,2), 10)[:d_len]\n","    return sample\n","\n","datasets = [\n","        Dataset(\n","            process_arc_sample(sample, 'input'),\n","            process_arc_sample(sample, 'output'),\n","            \n","            metadata = sample[\"filename\"]\n","        )\n","    for sample in dataset]\n","training_set = D(\n","    datasets[:-1]\n",")\n","training_set.batch_size = 1\n","testing_set = D(\n","    [datasets[-1]]\n",")\n","\n","# for idx, i in enumerate(training_set):\n","#     print(idx)\n","#     print(torch.argmax(i[0], -1), torch.argmax(i[1], -1))\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"executionInfo":{"elapsed":2634,"status":"error","timestamp":1666709143193,"user":{"displayName":"Liam Maclean","userId":"16471805810852624957"},"user_tz":240},"id":"O646_XlilAVb","outputId":"b18585d5-d1e1-41f8-e056-cc0d5b35ed92"},"outputs":[],"source":["\n","n= 5\n","\n","\n","heads = 1\n","hidden_dim = 64\n","edge_dim = None\n","# edge_dim = 8\n","\n","n_inputs = training_set.n_inputs\n","n_outputs = training_set.n_outputs\n","height = 3\n","width = training_set.n_inputs\n","\n","cuda_device = torch.device(\"cuda:0\" if torch.cuda.is_available else \"cpu\")\n","# cuda_device = torch.device(\"cpu\")\n","\n","update_rule = UpdateRule(\n","    n_inputs, \n","    n_outputs,\n","    hidden_dim,\n","    edge_dim,\n","    network_width=hidden_dim,#hidden_dim,\n","    input_vector_size = 10,\n","    heads = heads,\n","    cuda_device=cuda_device\n",")\n","# update_rule.build_graph(\n","#     height,\n","#     width,\n","#     mode=\"grid\",\n","#     input_mode=\"grid\",\n","#     n_edge_switches=0\n","# )\n","update_rule.build_graph_3d(\n","    (3,3),\n","    height\n",")\n","\n","update_rule = update_rule.to(cuda_device)\n","edge_attr = None\n","\n","update_rule.draw()\n","\n","#load initial_meta.pt\n","# update_rule.load_state_dict(torch.load(\"E52.pt\", map_location=cuda_device))\n","seed(12345)\n","\n","print(count_parameters(update_rule))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eaZnfyXSlAVh"},"outputs":[],"source":["from training import *\n","import time\n","n_steps = 6\n","batch_size = 32\n","training_set.batch_size = batch_size\n","lr = 0.001\n","optimizer = torch.optim.Adam(update_rule.parameters(), lr=lr)\n","use_wandb = True\n","\n","wandb_name = \"E54: pos encoding2\"\n","if use_wandb:\n","    wandb.init(\n","        project=\"Experiments\",\n","        name=wandb_name,\n","        # resume=True,\n","        tags=[\"Arc3x3\", \"batch_size=32\"]\n","        )\n","    # wandb.run.log_code(\".\")\n","\n","start_time = time.time()\n","train_on_meta_set(\n","    update_rule, optimizer, training_set, testing_set, {\n","        \"n_steps\": n_steps,\n","        \"batch_size\": batch_size,\n","        \"n_epochs\": 10000000,\n","        \"n_edge_switches\": 0,\n","    }, edge_attr=edge_attr,# edge_index=best_edges,\n","    # wandb_loss = f\"baseline loss\",\n","    # wandb_acc = f\"baseline acc\",\n","    wandb_log = use_wandb,\n","    save_dir=None,#\"Edge_Weight\",\n","    last_idx = 3,\n","    device=cuda_device,\n",")\n","# if use_wandb:\n","#     wandb.run.log_code(\".\")\n","#     wandb.finish()\n","\n","print(f\"Training took {time.time() - start_time} seconds\")\n","assert False, \"stop here\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torch.save(update_rule.state_dict(), \"E52.pt\")"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test = datasets[6  ]\n","# test = Dataset(test_dataset.data[0:4], test_dataset.target[0:4])\n","# test = Dataset(test.data[0:4], test.target[0:4])\n","\n","correct = test.target[-1].clone()\n","# test.target[-1] = torch.zeros_like(test.target[-1])\n","\n","# data = D(datasets ,shuffle=False)\n","data = D([test] ,shuffle=False)\n","# data = training_set\n","\n","update_rule.load_state_dict(torch.load(\"test3d.pt\", map_location=cuda_device))\n","\n","batch_size = 1\n","\n","data.batch_size = batch_size\n","\n","loader = DataLoader([update_rule.graph]*batch_size, batch_size = batch_size)\n","graph = loader.__iter__().__next__()\n","graph.batch = graph.batch.to(cuda_device)\n","x = update_rule.initial_state().repeat(batch_size, 1)\n","edge_index = update_rule.get_batch_edge_index(batch_size=batch_size, n_edge_switches=0)\n","update_rule.reset()\n","\n","training_set.batch_size = batch_size\n","\n","\n","\n","# training_set.batch_size=32\n","_, test_loss, network_output, correct, _, metadata = update_rule.eval()(\n","                x, 6, data, batch=graph.batch,\n","                edge_attr=edge_attr, edge_index=edge_index, last_idx = 3,\n","            )\n","\n","torch.argmax(torch.tensor(network_output),-1)[0], torch.argmax(torch.tensor(correct),-1)[0], test_loss, metadata"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data = D([test] ,shuffle=False)\n","data.batch_size = 1\n","for i in data:\n","    print(torch.argmax(i[0], -1), torch.argmax(i[1], -1))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#save update_rule\n","# torch.save(update_rule.state_dict(), \"E51.pt\")\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from model import *\n","count_parameters(SelfAttnAggregation(16, heads).attention1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["edge_index = update_rule.get_batch_edge_index(batch_size=5, n_edge_switches=0)\n","\n","graph = Data(edge_index=edge_index)\n","graph = utils.to_networkx(graph, to_undirected=True, remove_self_loops = True)\n","nx.draw(graph)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["datasets[:-1]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for d in datasets:\n","    print(len(d))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"8645a3289b338b219b448912b5fab165e90ac259118b71d23942463e3b992b2d"}}},"nbformat":4,"nbformat_minor":0}
